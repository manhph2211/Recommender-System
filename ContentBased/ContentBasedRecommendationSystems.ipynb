{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-Based Recommender Systems (CBRS) use item attributes to computing predictions. They try to match users to items that are similar to what they have liked in the past. This similarity is not neccessary based on rating correlations across users but on the basis of the attributes of the objects liked by the user. <br>\n",
    "CBRS focus on the target user's own ratings and the attributes of the items liked by user. Therefore, other users play no role in CBRS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBRS are dependent on two sources of data:\n",
    "1. Description of various items in terms of content-centric attributes (text descriptions of an item).\n",
    "2. User profile, which is generated from user feedback about various items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic components of Content-Based Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing and feature extraction\n",
    "2. Content-Based leraning of user profiles\n",
    "3. Filtering and recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Representation and Cleaning\n",
    "Keywords are converted int a vector-space representation:\n",
    "1. Stop-word removal\n",
    "2. Stemming\n",
    "3. Phrase extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating User Profile\n",
    "The data about user can be gotten from:\n",
    "1. Ratings\n",
    "2. Implicit feedback\n",
    "3. Text opinions\n",
    "4. Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Feature Selection and Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini index\n",
    "Let $t$ be the total number of possible values of the ratings. Among documents containning a particular word $w$, let $p_1(w)...p_t(w)$ be the fraction of item rated at each of these $t$ possible values. Then the Gini index of the word $w$ is definded as follows:\n",
    "$$ Gini(w) = 1 - \\sum_{i=1}^t p_i(w)^2 $$ <br>\n",
    "Small values being indicative of greater discriminative power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "$$ Entropy(w) = -\\sum_{i=1}^t p_i(w)log(p_i(w)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\chi^2$ Statistic\n",
    "Let $O_i$ be the observed value of the $i^{th}$ cell and $E_i$ be the expected value of the $i^{th}$ cell. Then the $\\chi^2$ Statistic is computed as follows:\n",
    "$$ \\chi^2 = \\sum_{i=1}^p \\dfrac{(O_i - E_i)^2}{E_i} $$\n",
    "$$ \\chi^2 = \\dfrac{(O_1 + O_2 + O_3 + O_4)(O_1O_4 - O_2O_3)}{(O_1 + O_2)(O_3 + O_4)(O_1 + O_3)(O_2 + O_4)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning User Profiles and Filtering\n",
    "We can use Nearest Neighbor Classification. But in this notebook, we just only use Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D_L$ containing the training documents, $D_U$ containing the test documents. <br>\n",
    "We assume that the label are binary in which users specify either a like or a dislike rating as $1$ and $-1$, the rating of the $i^{th}$ document in $D_L$ is denoted by $c_i \\in {-1, 1}$ <br>\n",
    "Each document is treated as a binary vector of $d$ word containing only values of $0$ and $1$. <br>\n",
    "Consider a target document $\\bar{X} \\in D_U$ are denoted by $(x_1...x_d)$. We would like to determine $P(Active\\ user\\ likes\\ \\bar{X}|x_1...x_d)$. Then we need to determine the value of $ P(c(\\bar{X}) = 1|x_1...x_d) $ and $ P(c(\\bar{X}) = -1|x_1...x_d) $, then select the larger of the two, one can determnine whether or not the active user likes $\\bar{X}$. We apply the Bayes rule as follows:\n",
    "$$ P(c(\\bar{X}) = 1|x_1...x_d) = \\dfrac{P(c(\\bar{X}) = 1).P(x_1...x_d|c(\\bar{X}) = 1)}{P(x_1...x_d)} $$\n",
    "$$ \\propto \\dfrac{P(c(\\bar{X}) = 1).P(x_1...x_d|c(\\bar{X}) = 1)}{P(x_1...x_d)} $$\n",
    "$$ = P(c(\\bar{X}) = 1).\\Pi_{i=1}^dP(x_i|c(\\bar{X}) = 1) $$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(c(\\bar{X}) = 1) = \\dfrac{|D_L^+| + \\alpha}{|D_L| + 2\\alpha} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional feature probability P(x_i|c(\\bar{X}) = 1) is estimated as the fraction of the instances in the positive class for which the $i^{th}$ feature take on the value of $x_i$. Let $q^+(x_i)$ represent the number of instances on the positive class that take on the value of $ x_i \\ in \\{0, 1\\}$ for the $i^{th}$ feature. Then:\n",
    "$$ P(x_i|c(\\bar{X}) = 1) = \\dfrac{q^+(x_i) + \\beta}{|D_L^+| + 2\\beta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie Lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[1, 1, 1, 0, 0, 0],\n",
    "                  [1, 1, 0, 0, 0, 1],\n",
    "                  [0, 1, 1, 0, 0, 0], \n",
    "                  [0, 0, 0, 1, 1, 1],\n",
    "                  [0, 1, 0, 1, 0, 1],\n",
    "                  [0, 0, 0, 1, 1, 0]])\n",
    "\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "x_test = np.array([[0, 0, 0, 1, 0, 0],\n",
    "                   [1, 0, 1, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-based classifiers can be designed in a variety of ways, including leave-one-out methods, as well as associative methods. For example:\n",
    "- Item contains keyword set A $\\Rightarrow$ $Rating = Like$\n",
    "- Item contains keyword set B $\\Rightarrow$ $Rating = Dislike$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall approach for rule-based classification can be described as follows:\n",
    "1. $\\textbf{Training phase: }$ Determine all the relevant rules from the user profile at the desired level of minimum support adn confidence from the training dataset $D_L$\n",
    "2. $\\textbf{Testing phases: }$ For each item description in $D_U$, determine the fired rules and an average rating. Rank the items in $D_U$ on the basis of this average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def findsubsets(S,m):\n",
    "    return set(itertools.combinations(S, m))\n",
    "\n",
    "def get_all_subset(input_list):\n",
    "    res_list = []\n",
    "    for i in range(len(input_list)):\n",
    "        r_i = list(findsubsets(input_list, i+1))\n",
    "        res_list += r_i\n",
    "    return res_list\n",
    "\n",
    "def get_number_of_occurrences(binary_matrix, index):\n",
    "    count = 0\n",
    "    for arr in binary_matrix:\n",
    "        if np.sum(arr[list(index)] != 1) == 0:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "def get_support(binary_matrix, index):\n",
    "    occurrences = get_number_of_occurrences(binary_matrix, index)\n",
    "    \n",
    "    return occurrences/len(binary_matrix)\n",
    "\n",
    "def get_confidence(binary_matrix, true_labels, label, index):\n",
    "    count = 0\n",
    "    contain_x = 0\n",
    "    for i in range(len(binary_matrix)):\n",
    "        arr = binary_matrix[i]\n",
    "        \n",
    "        if np.sum(arr[list(index)] != 1) == 0:\n",
    "            contain_x += 1\n",
    "            if true_labels[i] == label:\n",
    "                count += 1\n",
    "    if contain_x == 0:\n",
    "        return 0\n",
    "    return count/contain_x\n",
    "    \n",
    "def get_rules(binary_matrix, true_labels, min_support, min_confidence):\n",
    "    res = []\n",
    "    \n",
    "    indexes_range = range(len(binary_matrix[0]))\n",
    "    indexes = get_all_subset(indexes_range)\n",
    "    \n",
    "    for index in indexes:\n",
    "        for label in range(2):\n",
    "            support = get_support(binary_matrix, index)\n",
    "            confidence = get_confidence(binary_matrix, true_labels, label, index)\n",
    "            \n",
    "            if support >= min_support and confidence >= min_confidence:\n",
    "                res.append((index, label, support, confidence))\n",
    "#                 print(index, \" => \", label, \"\\t ===> support : \", support, \"\\t confidence: \", confidence)\n",
    "    res = sorted(res, key=lambda tup: (tup[3], tup[2]), reverse=True)\n",
    "    return res\n",
    "# get_rules(x_train, y_train, 0.33, 0.75)\n",
    "\n",
    "def get_rule_in_text(binary_matrix, true_labels, min_support, min_confidence,\n",
    "                     attribute_names = np.array(['Drums', 'Guitar', 'Beat', 'Classical', 'Symphony', 'Orchestra']),\n",
    "                     label_names = ['Dislike', 'Like']):\n",
    "    res = get_rules(binary_matrix, true_labels, min_support, min_confidence)\n",
    "    for r in res:\n",
    "        index, label, support, confidence = r\n",
    "        print(attribute_names[list(index)], \" => \", label_names[label], \"\\t ===> support : \", support, \"\\t confidence: \", confidence)\n",
    "        \n",
    "rules = get_rules(x_train, y_train, 0.33, 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  0  fired by rule  1\n",
      "Test  1  fired by rule  2\n",
      "Test  1  fired by rule  3\n"
     ]
    }
   ],
   "source": [
    "def check_fired_rules(test_matrix, rules):\n",
    "    for i in range(len(test_matrix)):\n",
    "        arr = test_matrix[i]\n",
    "        for j in range(len(rules)):\n",
    "            index, label, support, confidence = rules[j]\n",
    "            if np.sum(arr[list(index)] != 1) == 0:\n",
    "                print(\"Test \", i, \" fired by rule \", j + 1)\n",
    "                \n",
    "check_fired_rules(x_test, rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\bar{y}$ be the $n$-dimensional column vector containing the ratings of the actove user for the $n$ documents in the training set. The basic idea in linear regression is to assume that the ratings can be modeled as a linear function of the word frequencies. Let $\\bar{W}$ be a $d$-dimensional row vector representing the coefficients of each word in the linear function relating word frequencies to the rating:\n",
    "$$ \\bar{y} = D_L\\bar{W}^T $$\n",
    "The objective function $O$ can be expressed as follows:\n",
    "$$  Minimize\\ O = \\|D_L\\bar{W}^T - \\bar{y}\\|^2 + \\lambda\\|\\bar{W}\\|^2 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage\n",
    "1. Content-based systems have cold-start problems only for new users\n",
    "2. Content-based systems provide explanations in terms of the features of items.\n",
    "3. Content-based methods can generally be used with off-the-shelf text classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantage\n",
    "1. Content-based systems tend to find items that are similar to those the user has seen so far (overspecialization). It is always desirable to have a certain amount of novelty and serendipity in the recommendations.\n",
    "2. Content-based systems do not help in resolving the problem for new users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
