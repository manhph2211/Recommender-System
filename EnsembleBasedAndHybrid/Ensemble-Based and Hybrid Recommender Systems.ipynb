{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we would like to make use of all the knowledge available in different data sources and also use the algorithmic power of various recommender systems to make robust inferences - **Hybrid Recommender Systems**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three primary ways of creating hybrid recommender systems:\n",
    "1. **Ensemble design**: Results from off-the-shelf algorithms are combined into a single and more robust output.\n",
    "2. **Monolithic design**: An integrated recommendation algorithm is created by using various data types.\n",
    "3. **Mixed systems**: These systems use multiple recommendation algorithms as black-boxes, but the items recommended by the various systems are presented togetther side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **hybrid system** is used in a broader context than the term **ensemble system**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **hybrid recommendation systems** can be classified into the following categories:\n",
    "1. **Weighted**: The scores of several recommender systems are combined into a sigle unified score by computing the weighted aggregates of the scores from invidual ensemble components.\n",
    "2. **Switching**: The algorithm switches between various recommender systems depending on current needs.\n",
    "3. **Cascade**: One recommender system refines the recommendations given by another.\n",
    "4. **Feature augumentation**: The output of one recommender system is used to create input features for the next.\n",
    "5. **Feature combination**: The features from different data sources are combined and used in the contexr of a single recommender system.\n",
    "6. **Meta-level**: The model used by one recommender system is used as input to another system. Example: Content-based system creates peer groups, then the collaborative filtering system use that peer groups to make the recommendations.\n",
    "7. **Mixed**: Recommendations from several engines are presented to the user at the same time.\n",
    "\n",
    "The first four are ensemble systems, the next two are monolithic systems, and the last one is mixed system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error of a classifier in predicting the dependent variable can be decomposed into three components:\n",
    "1. **Bias**: Every classifier makes its own modeling assumptions about the nature of the decision boundary between classes. When a classifier has high bias, it will make consistently incorrect predictions over particular choices of test instances near the incorrectly modeled-decision boundary.\n",
    "2. **Variance**: Random variations in the choices of the training data will lead to different models. As a result, the dependent variable for a test instance might be inconsistently predicted by different choices of training data sets. Model variance is closely related to overfitting.\n",
    "3. **Noise**: The noise refers to the intrinsic errors in the target class labeling.\n",
    "\n",
    "The expected mean-squared error of a classifier over a set of test instances can be shown to be sum of the bias, variance, and noise:\n",
    "$$ Error = Bias^2 + Variance + Noise $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification ensemble methods such as *bagging* reduce the variance, whereas methods such as *boosting* can reduce the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $R = [r_{uj}]$ be an $m \\times n$ ratings matrix. Let $\\hat{R}_1...\\hat{R}_q$ be the $m \\times n$\n",
    "completely specified ratings matrices, in which the unobserved entries of $R$ are predicted by $q$ different algorithms. Then, for a set of weights $\\alpha_1...\\alpha_q$ , the weighted hybrid creates a combined prediction matrix $\\hat{R} = [\\hat{r}_{uj}]$ as follows:\n",
    "$$ \\hat{R} = \\sum_{i=1}^q \\alpha_i \\hat{R}_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_all_predicted_ratings_matrix(list_matrixs, list_weights):\n",
    "    z = np.multiply(list_weights, np.transpose(list_matrix))\n",
    "    z = np.transpose(z)\n",
    "    \n",
    "    return np.sum(z, axis = 0)/np.sum(list_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the optimal weights, it is necessary to be able to evaluate the\n",
    "effectiveness of a particular combination of weights $\\alpha_1...\\alpha_q$. <br>\n",
    "A simple approach is to hold out a small fraction (e.g., 25%) of the known entries in the $m \\times n$ ratings matrix $R = [r_{uj}]$ and create the prediction matrices $\\hat{R}_1...\\hat{R}_q$ by applying the $q$ different base algorithms on the remaining 75% of the entries in R. The resulting predictions $\\hat{R}_1...\\hat{R}_q$ are then combined to create the ensemble-based prediction $\\hat{R}$. Let the user-item indices $(u,j)$ of these held-out entries be denoted by $H$. The effectiveness of a particular scheme can be evaluated using either the mean-squared error (MSE) or the mean absolute error (MAE) of the predicted matrix over the held-out ratings in $H$. We can use linear regression model to find the most effective values of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the weights have been learned using linear regression, the individual component models are retrained on the entire training set without any held-out entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization can be added to prevent overfitting. It is also possible to add other con- straints on the various values of Î±i such as non-negativity or ensuring that they sum to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, these tech- niques use a simple average of the predictions of different components. It is particularly important to weight the different components when the predicted utility values are on dif- ferent scales, or when some of the ensemble components are much more accurate than others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices for vector\n",
    "def specified_rating_indices(u):\n",
    "    return np.where(np.isfinite(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def get_batch(X, y, batch_size, iteration):\n",
    "    return (X[iteration * batch_size: (iteration + 1) * batch_size], y[iteration * batch_size: (iteration + 1) * batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weights_using_linear_regression(list_predicted_matrix, original_matrix,\n",
    "                                         batch_size=2, n_epochs=10, learning_rate=0.001):\n",
    "    indices = specified_rating_indices(original_matrix)\n",
    "\n",
    "    X_train = []\n",
    "    for i in range(len(list_predicted_matrix)):\n",
    "        matrix = list_predicted_matrix[i]\n",
    "        X_train.append(matrix[indices])\n",
    "        \n",
    "    X_train = np.transpose(X_train)\n",
    "    y_train = original_matrix[indices]\n",
    "    \n",
    "    n_dimensions = len(list_predicted_matrix)\n",
    "    \n",
    "    # create tensorflow graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_dimensions], name='X')\n",
    "    y = tf.placeholder(tf.float32, shape=None, name='y')\n",
    "    \n",
    "#     W = tf.get_variable('W', shape=[n_dimensions, 1], initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b = tf.Variable(dtype=tf.float32, initial_value=0)\n",
    "#     output = tf.matmul(X, W) + b\n",
    "\n",
    "    output = tf.layers.dense(X, units=1)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(output, y)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "            total_loss = 0\n",
    "            n_iteration = X_train.shape[0] // batch_size\n",
    "            for i in range(n_iteration):\n",
    "                X_batch, y_batch = get_batch(X_train, y_train, batch_size, i)\n",
    "                l, _ = sess.run([loss, optimizer], feed_dict={X: X_batch, y: y_batch})\n",
    "                total_loss += l\n",
    "            \n",
    "            print('Loss: ', total_loss / n_iteration)\n",
    "        \n",
    "        var = [v for v in tf.trainable_variables()][0]\n",
    "        value = sess.run(var)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  19.537912786006927\n",
      "Loss:  15.251693665981293\n",
      "Loss:  11.021173030138016\n",
      "Loss:  8.004278674721718\n",
      "Loss:  7.621613174676895\n",
      "Loss:  7.934566304087639\n",
      "Loss:  6.653058409690857\n",
      "Loss:  8.155106753110886\n",
      "Loss:  7.148072227835655\n",
      "Loss:  6.631481006741524\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 9, 4, 5],\n",
    "              [nan, 1, 3, nan, 2]])\n",
    "b = np.array([[2, 2, 4, 5, 6],\n",
    "              [nan, 1, 3, nan, 2]])\n",
    "c = np.array([[3, 4, 5, 6, 7],\n",
    "              [nan, 1, 3, nan, 2]])\n",
    "\n",
    "value = find_weights_using_linear_regression([b, c], a, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0894009 ],\n",
       "       [-0.20308039]], dtype=float32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Types of Model Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are typically two forms of model combinations:\n",
    "1. **Homogeneous data type and model classes**: Different models are applied on the same data. Such an approach is robust because it avoids the specific bias of particular algorithms on a given data set even though all the constituent models belong to the same class\n",
    "2. **Heterogeneous data type and model classes**: Different classes of models are applied to different data sources. The idea is to leverage the complementary knowledge in the various data sources in order to provide the most accurate recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting Bagging from Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
