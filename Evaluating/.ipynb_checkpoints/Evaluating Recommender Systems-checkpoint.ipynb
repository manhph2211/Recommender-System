{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proper design of the evaluation system is crucial in order to obtain an understanding of the effectiveness of various recommendation algorithms. An incorrect design of the experimental evaluation can lead to either gross underestimation or overestimation of the true accuracy of a particular algorithm or model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems can be evaluated using either **online methods** or **offline methods**. <br>\n",
    "- In an online system, the user reactions are measured with respect to the presented recommendations. Therefore, user participation is essential in online systems. <br>\n",
    "- Offline evaluations with historical data sets are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, the following issues are important from the perspective of designing evaluation methods for recommender systems:\n",
    "1. **Evaluation goals**: Although accuracy metrics are arguably the most important components of the evaluation, many secondary goals such as novelty, trust, coverage, and serendipity are important to the user experience. This is because these metrics have important short- and long-term impacts on the conversion rates.\n",
    "2. **Experimental design issues**: Even when accuracy is used as the metric, it is crucial to design the experiments so that the accuracy is not overestimated or underestimated.\n",
    "3. **Accuracy metrics**: Recommender systems can be evaluated either in terms of the prediction accuracy of a rating or the accuracy of ranking the items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Paradigms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three primary types of evaluation of recommender systems, corresponding to user studies, online evaluations, and offline evaluations with historical data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Studies\n",
    "In user studies, test subjects are actively recruited, and they are asked to interact with the recommender system to perform specific tasks. Feedback can be collected from the user before and after the interaction, and the system also collects information about their interaction with the recommender system. These data are then used to make inferences about the likes or dislikes of the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Evaluation\n",
    "Online evaluations also leverage user studies except that the users are often real users in a fully deployed or commercial system. <br>\n",
    "A typical example of a metric, which is used to measure the effectiveness of the recommender system on the users, is the *conversion rate*. The conversion rate measures the frequency with which a user selects a recommended item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods are also referred to as A/B testing, and they measure the direct impact of the recommender system on the end user. The basic idea in these methods is to compare two algorithms as follows:\n",
    "1. Segment the users into two groups A and B.\n",
    "2. Use one algorithm for group A and another algorithm for group B for a period of time, while keeping all other conditions (e.g., selection process of users) across the two groups as similar as possible.\n",
    "3. At the end of the process, compare the conversion rate (or other payoff metric) of the two groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Evaluation with Historical Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In offline testing, historical data, such as ratings, are used. In some cases, temporal information may also be associated with the ratings, such as the time-stamp at which each user has rated the item. <br>\n",
    "Offline methods are among the most popular techniques for testing recommendation algorithms, because standardized frameworks and evaluation measures have been developed for such cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Goals of Evaluation Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "The main components of accuracy evaluation are as follows:\n",
    "1. **Designing the accuracy evaluation**\n",
    "2. **Accuracy metrics**: Accuracy metrics are used to evaluate either the prediction accuracy of estimating the ratings of specific user-item combinations or the accuracy of the top- k ranking predicted by a recommender system.\n",
    "    - Accuracy of estimating ratings: MSE, RMSE, MAE,...\n",
    "    - Accuracy of estimating rankings: Depending on the nature of the ground-truth, one can use rank-correlation measures, utility-based measures, or the receiver operating characteristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some measures of accuracy are also designed to maximize the profit for the merchant because all items are not equally important from the perspective of the recommendation process. These metrics incorporate item-specific costs into the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage\n",
    "\n",
    "Even when a recommender system is highly accurate, it may often not be able to ever recommend a certain proportion of the items, or it may not be able to ever recommend to a certain proportion of the users. This measure is referred to as coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence and Trust\n",
    "\n",
    "The estimation of ratings is an inexact process that can vary significantly with the specific training data at hand. Furthermore, the algorithmic methodology might also have a significant impact on the predicted ratings. This always leads to uncertainty in the user about the accuracy of the predictions. Many recommender systems may report ratings together with confidence estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novelty\n",
    "The novelty of a recommender system evaluates the likelihood of a recommender system to give recommendations to the user that they are not aware of, or that they have not seen before.  Unseen recommendations often increase the ability of the user to discover important insights into their likes and dislikes that they did not know previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, all ratings that were created after a certain point in time t0 are removed from the training data. Furthermore, some of the ratings occurring before t0 are also removed. The system is then trained with these ratings removed. These removed items are then used for scoring purposes. For each item rated before time t0 and correctly recommended, the novelty evaluation score is penalized. On the other hand, for each item rated after time t0 and correctly recommended, the novelty evaluation score is rewarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serendipity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word “serendipity” literally means “lucky discovery.” Therefore, serendipity is a measure of the level of surprise in successful recommendations. In other words, recommendations need to be unexpected. In contrast, novelty only requires that the user was not aware of the recommendation earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of diversity implies that the set of proposed recommendations within a single recommended list should be as diverse as possible. Note that the diversity is always measured over a set of recommendations, and it is closely related to novelty and serendipity. Ensuring greater diversity can often increase the novelty and serendipity of the recommendations. Furthermore, greater diversity of recommendations can also increase the sales diversity and catalog coverage of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness and Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recommender system is stable and robust when the recommendations are not significantly affected in the presence of attacks such as fake ratings or when the patterns in the data evolve significantly over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variety of measures are used for determining the scala- bility of a system:\n",
    "1. Training time\n",
    "2. Prediction time\n",
    "3. Memory requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Issues in Offline Recommender Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial to design recommender systems in such a way that the accuracy is not grossly overestimated or underestimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are often divided into three parts:\n",
    "1. **Training data**: This part of the data is used to build the training model\n",
    "2. **Validation data**: This part of the data is used for model selection and parameter tuning.\n",
    "3. **Testing data**: This part of the data is used to test the accuracy of the final (tuned) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenting the Ratings for Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out\n",
    "In the hold-out method, a fraction of the entries in the ratings matrix are hidden, and the remaining entries are used to build the training model. The accuracy of predicting the hidden entries is then reported as the overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "In the cross-validation method, the ratings entries are divided into $q$ equal sets. One of the $q$ segments is used for testing, and the\n",
    "remaining $(q − 1)$ segments are used for training. This process is repeated $q$ times by using each of the $q$ segments as the test set.\n",
    "The average accuracy over the q different test sets is reported. In practice, the value of q is fixed to a number such as 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Classification Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference from classification design is that the performance on hidden entries often does not reflect the true performance of the system in real settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Metrics in Offline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Accuracy of Ratings Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S$ be the set of specified (observed) entries, and $E \\subset S$ be the set of entries in the test set used for evaluation. <br>\n",
    "Let $r_{uj}$ be the value of the (hidden) rating of entry $(u, j) \\in E$, which is used in the test set. Furthermore, let $\\hat{r}_{uj}$ be the predicted rating of the entry $(u, j)$ by the specific training\n",
    "algorithm being used. The entry-specific error is given by $e_{uj} = \\hat{r}_{uj} − r_{uj}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean square error\n",
    "\n",
    "$$ MSE = \\dfrac{\\sum_{(u, j) \\in E} e_{uj}^2}{|E|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root mean square error\n",
    "\n",
    "$$ RMSE = \\sqrt{\\dfrac{\\sum_{(u, j) \\in E} e_{uj}^2}{|E|}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean absolute error\n",
    "\n",
    "$$ MAE = \\dfrac{\\sum_{(u, j) \\in E} |e_{uj}|}{|E|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize root mean square error\n",
    "\n",
    "$$ NRMSE = \\dfrac{RMSE}{r_{max} - r_{min}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize mean absolute error\n",
    "\n",
    "$$ NRMSE = \\dfrac{MAE}{r_{max} - r_{min}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Ranking via Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the recommender system creates a ranking of items for a user, and the $top-k$ items are recommended. In general, it is desirable for highly rated items to be ranked above items which are not highly rated. Consider a user $u$, for which the ratings of the set $I_u$ of items have been hidden by a hold-out or cross-validation strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to measure how well the ground-truth orderings of the ratings in $I_u$ are related to the ordering predicted by the recommender system for the set $I_u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common class of methods is to use rank correlation coefficients. The two most commonly used rank correlation coefficients are as follows:\n",
    "1. **Spearman rank correlation coefficient**: The first step is to rank all items from $1$ to $|I_u|$, both for the recommender system prediction and for the ground-truth. The Spearman correlation coefficient is simply equal to the Pearson correlation coefficient applied on these ranks. The computed value always ranges in $(−1, +1)$, and large positive values are more desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.8720815992723809, pvalue=0.05385421772754211)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank of all items\n",
    "spearmanr([2,3,4,5,6],[5,6,6,8,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Kendall rank correlation coefficient**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4714045207910316\n",
      "0.2827454599327748\n"
     ]
    }
   ],
   "source": [
    "x1 = [12, 2, 1, 12, 2]\n",
    "x2 = [1, 4, 7, 1, 0]\n",
    "tau, p_value = kendalltau(x1, x2)\n",
    "print(tau)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Ranking via Utility "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility-based methods use the ground-truth rating in combination with the recommender system’s ranking. For the case of implicit feedback data sets, the rating is substituted with a 0-1 value, depending on whether or not the customer has consumed the item. The overall goal of utility-based methods is to create a crisp quantification of how useful the customer might find the recommender system’s ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility-based measures quantify the utility of a recommendation list\n",
    "by giving greater importance to the top-ranked items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In utility-based ranking, the basic idea is that each item in $I_u$ has a utility to the user, which depends both on its position in the recommended list and its ground-truth rating.\n",
    "An item that has a higher ground-truth rating obviously has greater utility to the user.\n",
    "Furthermore, items ranked higher in the recommended list have greater utility to the user i because they are more likely to be noticed (by virtue of their position) and eventually selected. Ideally, one would like items with higher ground-truth rating to be placed as high on the recommendation list as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are these rating-based and ranking-based components defined? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any item $j \\in I_u$:\n",
    "- Its rating-based utility to the user i is assumed to be $max\\{r_{uj} − C_u, 0\\}$, where $C_u$ is a break-even (neutral) rating value for user $u$.\n",
    "- The ranking-based utility of the item is $2^{−(v_j−1)/\\alpha}$ , where $v_j$ is the rank of item $j$ in the list of recommended items and $\\alpha$ is a half-life parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility $F(u, j)$ of item $j \\in I_u$ to user $u$ is defined as the product of the rating-based and ranking-based utility values:\n",
    "$$ F(u, j) = \\dfrac{max\\{r_{uj} − C_u, 0\\}}{2^{(v_j−1)/\\alpha}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-score, which is specific to user $u$, is the sum of $F(u, j)$ over all the hidden ratings in $I_u$:\n",
    "\n",
    "$$ R_{score}(u) = \\sum_{j \\in I_u} F(u, j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, one often restricts the size of the recommended list to a maximum value of L. One can therefore compute the R-score over a recommended list of specific size L instead of using all the items, as follows:\n",
    "$$  R_{score}(u) = \\sum_{j \\in I_u, v_j \\leq L} F(u, j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall R-score may be computed by summing this value over all the users:\n",
    "$$ R_{score} = \\sum_{u=1}^m R_{score}(u) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponential decay in the utility implies that users are only interested in top-ranked items, and they do not pay much attention to lower-ranked items. This may not be true in all applications. The discounted cumulative gain (DCG). In this case, the discount factor of item $j$ is set to $\\log_2 (v_j + 1)$, where $v_j$ is the rank of item $j$ in the test set $I_u$ . Then, the discounted cumulative gain is defined as follows:\n",
    "$$ DCG = \\dfrac{1}{m} \\sum_{u=1}^m \\sum_{j \\in I_u} \\dfrac{g_{uj}}{\\log_2 (v_j + 1)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, g uj represents the utility (or gain) of the user u in consuming item j. Typically, the value of g uj is set to an exponential function of the relevance (e.g., non-negative ratings or user hit rates):\n",
    "$$ g_{uj} = 2^{{rel}_{uj}} - 1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, rel uj is the ground-truth relevance of item j for user u, which is computed as a heuristic function of the ratings or hits. In many settings, the raw ratings are used. It is common to compute the discounted cumulative gain over a recommendation list of specific size L, rather than using all the items:\n",
    "$$ DCG = \\dfrac{1}{m} \\sum_{u=1}^m \\sum_{j \\in I_u, v_j \\leq L} \\dfrac{g_{uj}}{\\log_2 (v_j + 1)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the normalized discounted cumulative gain (NDCG) is defined as ratio of the discounted cumulative gain to its ideal value, which is also referred to as ideal discounted cumulative gain (IDCG).\n",
    "$$ NCDG = \\dfrac{DCG}{IDCG} $$\n",
    "The ideal discounted cumulative gain is computed by repeating the computation for DCG, except that the ground-truth rankings are used in the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 10\n",
    "n_items = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix = np.random.randint(5, size=(n_users, n_items)) + 1\n",
    "predicted_matrix = np.random.randint(5, size=(n_users, n_items)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 5, 5, 3, 5, 5, 5, 3, 3, 2],\n",
       "       [4, 4, 4, 2, 5, 1, 1, 1, 4, 1],\n",
       "       [1, 5, 3, 5, 1, 2, 3, 5, 3, 3],\n",
       "       [4, 2, 5, 2, 4, 1, 4, 1, 3, 2],\n",
       "       [4, 2, 3, 3, 2, 2, 3, 1, 3, 4],\n",
       "       [5, 4, 2, 3, 1, 2, 1, 1, 3, 4],\n",
       "       [1, 4, 4, 5, 5, 1, 1, 1, 2, 2],\n",
       "       [2, 4, 4, 2, 1, 5, 3, 2, 2, 3],\n",
       "       [1, 4, 4, 3, 2, 2, 4, 3, 3, 2],\n",
       "       [3, 2, 2, 5, 1, 1, 5, 2, 1, 3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2, 4, 5, 2, 3, 1, 4, 4, 5],\n",
       "       [4, 5, 5, 5, 2, 4, 4, 1, 3, 3],\n",
       "       [5, 4, 5, 5, 1, 2, 2, 5, 1, 4],\n",
       "       [3, 2, 4, 1, 4, 2, 2, 3, 4, 4],\n",
       "       [1, 5, 2, 5, 5, 4, 1, 2, 2, 5],\n",
       "       [2, 3, 2, 4, 2, 3, 3, 3, 4, 1],\n",
       "       [3, 5, 5, 1, 5, 4, 1, 1, 1, 1],\n",
       "       [4, 5, 1, 4, 2, 1, 2, 3, 1, 1],\n",
       "       [5, 4, 3, 1, 4, 3, 3, 1, 4, 1],\n",
       "       [1, 3, 3, 4, 2, 3, 5, 4, 4, 4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility(u, j):\n",
    "    mean_u = np.mean(rating_matrix[u])\n",
    "    rating_based = np.max([rating_matrix[u, j] - mean_u, 0])\n",
    "    \n",
    "    arg_sort = np.argsort(predicted_matrix[u])\n",
    "\n",
    "    rank = list(arg_sort).index(j) + 1\n",
    "    alpha = 2\n",
    "    ranking_based = np.power(2, -(rank - 1)/alpha)\n",
    "    \n",
    "    return rating_based * ranking_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363961030678931"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rscore_for_user(u):\n",
    "    r_score = 0\n",
    "    \n",
    "    for i in range(n_items):\n",
    "        if np.isnan(rating_matrix[u, i]):\n",
    "            continue\n",
    "        r_score += utility(u, i)\n",
    "        \n",
    "    return r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.842856236403074"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_rscore_for_user(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rscore_for_all_user():\n",
    "    r_score = 0\n",
    "    \n",
    "    for u in range(n_users):\n",
    "        r_score += calculate_rscore_for_user(u)\n",
    "        \n",
    "    return r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.91870984500038"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_rscore_for_all_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dcg():\n",
    "    sum = 0\n",
    "    \n",
    "    for u in range(n_users):\n",
    "        arg_sort = np.argsort(predicted_matrix[u])\n",
    "        for i in range(n_items):\n",
    "            rating = rating_matrix[u, i]\n",
    "            if np.isnan(rating):\n",
    "                continue\n",
    "            g = np.power(2, rating) - 1\n",
    "            \n",
    "            rank = list(arg_sort).index(i) + 1\n",
    "            sum += g/(np.log(rank+1)/np.log(2))\n",
    "    \n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491.15159123167416"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_dcg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Reciprocal Hit Rate (ARHR)\n",
    "This measure is designed for implicit feedback data sets, in which each value of $r_{uj} \\in {0, 1}$. In this implicit feedback setting, missing values in the ratings matrix are assumed to be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the rank-based discount rate is $1/v_j$, where $v_j$ is the rank of item $j$ in the recommended list. Therefore, the combined utility of an item is given by $r_{uj}/v_j$. This expression represents the contribution of item $j \\in I_u$ to the utility. Then, the ARHR metric for the user $i$ is defined by summing up these values over all the hidden items in $I_u$:\n",
    "$$ ARHR(u) = \\sum_{j \\in I_u} \\dfrac{r_{uj}}{v_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to define the average reciprocal hit-rate for a recommended list of size L\n",
    "by adding only those utility values for which vj ≤ L:\n",
    "$$ ARHR(u) = \\sum_{j \\in I_u, v_j \\leq L} \\dfrac{r_{uj}}{v_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global ARHR value is computed by averaging this value over the $m$ users:\n",
    "$$ ARHR = \\dfrac{\\sum_{u=1}^m ARHR(u)}{m} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_arhr_for_user(u):\n",
    "    arhr = 0\n",
    "    arg_sort = np.argsort(predicted_matrix[u])\n",
    "    for i in range(n_items):\n",
    "        rating = rating_matrix[u, i]\n",
    "        if np.isnan(rating):\n",
    "            continue\n",
    "            \n",
    "        rank = list(arg_sort).index(i) + 1\n",
    "        arhr += rating/rank\n",
    "    return arhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.337301587301588"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_arhr_for_user(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_arhr_for_all_user():\n",
    "    arhr = 0\n",
    "    \n",
    "    for u in range(n_users):\n",
    "        arhr += calculate_rscore_for_user(u)\n",
    "        \n",
    "    return arhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.448615834565942"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_arhr_for_all_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Ranking via Receiver Operating Characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking methods are used frequently in the evaluation of the actual consumption of items.<br>\n",
    "The items that are eventually consumed are also referred to as the **ground-truth positives** or **true positives**. The recommendation algorithm can provide a ranked list of any number of items. Changing the number of recommended items in the ranked list has a direct effect on the trade-off between the fraction of recommended items that are actually consumed and the fraction of consumed items that are captured by the recommender system. This trade-off can be measured in two different ways with the use of a precision-recall or a receiver operating characteristic (ROC) curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the size of the recommended list, one can then examine the fraction of relevant (ground-truth positive) items in the list, and the fraction of relevant items that are missed by the list. If the recommended list is too small, then the algorithm will miss relevant items (false-negatives). On the other hand, if a very large list is recommended, this will lead to too many spurious recommendations that are never used by the user (false-positives). This leads to a trade-off between the false-positives and false-negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that one selects the $top-t$ set of ranked items to recommend to the user. For any given value $t$ of the size of the recommended list, the set of recommended items is denoted by $S(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $G$ represent the true set of relevant items (ground-truth positives) that are consumed by the user. Then, for any given size t of the recommended list, the precision is defined as the percentage of recommended items that truly turn out to be relevant (i.e., consumed by the user):\n",
    "$$ Precision(t) = 100.\\dfrac{|S(t) \\cap G|}{|S(t)|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall is correspondingly defined as the percentage of ground-truth positives that have been recommended as positive for a list of size t:\n",
    "$$ Recall(t) = 100.\\dfrac{|S(t) \\cap G|}{|G|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-measure, which is the harmonic mean between the precision and the recall:\n",
    "$$ F_1(t) = \\dfrac{2.Precision(t).Recall(t)}{Precision(t) + Recall(t)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "\n",
    "A second way of generating the trade-off in a more intuitive way is through the use of the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true-positive rate, which is the same as the recall, is defined as the percentage of ground-truth positives that have been included in the recommendation list of size t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The false-positive rate FPR(t) is the percentage of the falsely reported positives in the recommended list out of the ground-truth negatives (i.e., irrelevant items not consumed by the user). U is universe of all items.\n",
    "$$ FPR(t) = 100.\\dfrac{|S(t) - G|}{|U - G|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is defined by plotting the $FPR(t)$ on the X-axis and $TPR(t)$ on the Y-axis for varying values of t."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
